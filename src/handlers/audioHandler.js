const languageService = require('../services/languageService');
const openaiService = require('../services/openaiService');
const databaseService = require('../services/databaseService');
const logger = require('../utils/logger');
const path = require('path');
const fs = require('fs-extra');
const axios = require('axios');
const { config } = require('../config/config');

class AudioHandler {

  /**
   * Handle voice messages
   */
  async handleVoice(ctx) {
    try {
      const userId = ctx.from.id;
      
      // Get user and settings
      const user = await databaseService.getUserByTelegramId(userId);
      if (!user) {
        await ctx.reply('‚ùå –ü–æ–º–∏–ª–∫–∞: –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –Ω–µ –∑–Ω–∞–π–¥–µ–Ω. –°–ø—Ä–æ–±—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /start');
        return;
      }

      const userSettings = user.languages;
      logger.info(`Processing voice message from user ${userId}`);

      // Get or create active chat with current language settings
      const languagePair = {
        from: userSettings.primaryLanguage,
        to: userSettings.secondaryLanguage
      };
      
      const activeChat = await databaseService.getOrCreateUserActiveChat(user._id, languagePair);
      
      // Send processing message
      const processingMsg = await ctx.reply('üé§ –û–±—Ä–æ–±–ª—è—é –≥–æ–ª–æ—Å–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è...\n‚è≥ –†–æ–∑–ø—ñ–∑–Ω–∞—é –º–æ–≤—É...');
      
      // Download audio file
      const audioPath = await this.downloadAudio(ctx);
      
      try {
        // Update processing message
        await ctx.telegram.editMessageText(
          ctx.chat.id, 
          processingMsg.message_id, 
          null, 
          'üé§ –û–±—Ä–æ–±–ª—è—é –≥–æ–ª–æ—Å–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è...\nüîÑ –ü–µ—Ä–µ–∫–ª–∞–¥–∞—é...'
        );

        // Check if user can make translation (token limits)
        const canTranslate = await databaseService.canUserMakeTranslation(user._id, 150);
        if (!canTranslate) {
          await ctx.telegram.editMessageText(
            ctx.chat.id,
            processingMsg.message_id,
            null,
            '‚ö†Ô∏è **–õ—ñ–º—ñ—Ç –≤–∏—á–µ—Ä–ø–∞–Ω–æ**\n\n–í–∏ –¥–æ—Å—è–≥–ª–∏ –¥–µ–Ω–Ω–æ–≥–æ –∞–±–æ –º—ñ—Å—è—á–Ω–æ–≥–æ –ª—ñ–º—ñ—Ç—É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è.\n\nüíé –†–æ–∑–≥–ª—è–¥–∞–π—Ç–µ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –ø—Ä–µ–º—ñ—É–º –ø—ñ–¥–ø–∏—Å–∫–∏ –¥–ª—è –Ω–µ–æ–±–º–µ–∂–µ–Ω–∏—Ö –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤!'
          );
          return;
        }

        // Process translation with automatic language detection
        const result = await openaiService.completeTranslationAuto(
          audioPath,
          userSettings.primaryLanguage,
          userSettings.secondaryLanguage
        );

        // Update processing message
        await ctx.telegram.editMessageText(
          ctx.chat.id, 
          processingMsg.message_id, 
          null, 
          'üé§ –û–±—Ä–æ–±–ª—è—é –≥–æ–ª–æ—Å–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è...\nüíæ –ó–±–µ—Ä—ñ–≥–∞—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç...'
        );

        // Save translation to database
        const translationData = {
          original: {
            text: result.original,
            language: result.detectedLanguage
          },
          translated: {
            text: result.translated,
            language: result.targetLanguage
          },
          backTranslation: result.backTranslation,
          tokensUsed: result.tokensUsed || 150,
          audioFile: {
            telegramFileId: ctx.message.voice.file_id,
            duration: ctx.message.voice.duration,
            size: ctx.message.voice.file_size
          }
        };

        await databaseService.addTranslationToChat(activeChat._id, translationData);
        
        // Add token usage to user
        await databaseService.addUserTokenUsage(user._id, translationData.tokensUsed);

        // Format and send result
        await this.sendTranslationResult(ctx, result, userSettings, activeChat);
        
        // Delete processing message
        await ctx.telegram.deleteMessage(ctx.chat.id, processingMsg.message_id);

      } finally {
        // Clean up audio file
        await this.cleanupAudioFile(audioPath);
      }

    } catch (error) {
      logger.error('Error processing voice message:', error);
      await ctx.reply('‚ùå –í–∏–Ω–∏–∫–ª–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑.');
    }
  }

  /**
   * Download audio file from Telegram
   */
  async downloadAudio(ctx) {
    try {
      const voice = ctx.message.voice;
      const fileId = voice.file_id;
      
      // Get file info from Telegram
      const fileInfo = await ctx.telegram.getFile(fileId);
      const fileUrl = `https://api.telegram.org/file/bot${config.telegram.token}/${fileInfo.file_path}`;
      
      // Generate unique filename
      const timestamp = Date.now();
      const fileName = `voice_${ctx.from.id}_${timestamp}.ogg`;
      const filePath = path.join(config.bot.tempAudioDir, fileName);
      
      // Ensure directory exists
      await fs.ensureDir(config.bot.tempAudioDir);
      
      // Download file
      const response = await axios({
        method: 'GET',
        url: fileUrl,
        responseType: 'stream'
      });
      
      const writer = fs.createWriteStream(filePath);
      response.data.pipe(writer);
      
      return new Promise((resolve, reject) => {
        writer.on('finish', () => {
          logger.info(`Audio file downloaded: ${filePath}`);
          resolve(filePath);
        });
        writer.on('error', reject);
      });

    } catch (error) {
      logger.error('Error downloading audio file:', error);
      throw error;
    }
  }

  /**
   * Send formatted translation result
   */
  async sendTranslationResult(ctx, result, userSettings, activeChat) {
    try {
      const detectedLang = languageService.getLanguageInfo(result.detectedLanguage);
      const targetLang = languageService.getLanguageInfo(result.targetLanguage);
      const primaryLang = languageService.getLanguageInfo(userSettings.primaryLanguage);
      const secondaryLang = languageService.getLanguageInfo(userSettings.secondaryLanguage);
      
      // Build language detection info if available
      let detectionInfo = '';
      if (result.whisperDetection && result.gptDetection) {
        const whisperLang = languageService.getLanguageInfo(result.whisperDetection);
        const gptLang = languageService.getLanguageInfo(result.gptDetection);
        
        if (result.whisperDetection === result.gptDetection) {
          detectionInfo = `\nüîç *–†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è:* Whisper —Ç–∞ GPT –ø–æ–≥–æ–¥–∏–ª–∏—Å—å –Ω–∞ ${detectedLang.flag} ${detectedLang.name}`;
        } else {
          detectionInfo = `\nüîç *–†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è:* Whisper: ${whisperLang.flag} ${whisperLang.name}, GPT: ${gptLang.flag} ${gptLang.name} ‚Üí –û–±—Ä–∞–Ω–æ: ${detectedLang.flag} ${detectedLang.name}`;
        }
      }
      
      const message = `üåç **${targetLang.flag} –ü–ï–†–ï–ö–õ–ê–î:**
**${result.translated}**

üé§ *–†–æ–∑–ø—ñ–∑–Ω–∞–Ω–æ* (${detectedLang.flag} ${detectedLang.name}):
${result.original}

üîÑ *–ó–≤–æ—Ä–æ—Ç–Ω—ñ–π –ø–µ—Ä–µ–∫–ª–∞–¥* (${detectedLang.flag} ${detectedLang.name}):
${result.backTranslation}

ü§ñ *–í–∞—à—ñ –º–æ–≤–∏:* ${primaryLang.flag} ${primaryLang.name} ‚áÑ ${secondaryLang.flag} ${secondaryLang.name}
üí¨ *–ß–∞—Ç:* ${activeChat.title} | üìä ${activeChat.stats.totalTranslations} –ø–µ—Ä–µ–∫–ª–∞–¥—ñ–≤${detectionInfo}

üí° –ü–æ–∫—Ä–∞—â–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Ä–æ–∑–ø—ñ–∑–Ω–∞–ª–∞ –º–æ–≤—É —Ç–∞ –ø–µ—Ä–µ–≤–µ–ª–∞ –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—É.`;

      const keyboard = {
        inline_keyboard: [
          [
            {
              text: `üîÑ ${primaryLang.flag} ‚áÑ ${secondaryLang.flag} Switch`,
              callback_data: 'switch_and_speak'
            }
          ],
          [
            {
              text: 'üí¨ –ù–æ–≤–∏–π —á–∞—Ç',
              callback_data: 'new_chat'
            },
            {
              text: 'üìö –Ü—Å—Ç–æ—Ä—ñ—è —á–∞—Ç—ñ–≤',
              callback_data: 'chat_history'
            }
          ],
          [
            {
              text: 'üìä –ú–æ—ó –ª—ñ–º—ñ—Ç–∏',
              callback_data: 'show_limits'
            }
          ]
        ]
      };

      await ctx.reply(message, {
        parse_mode: 'Markdown',
        reply_markup: keyboard
      });

      logger.info(`Translation sent for user ${ctx.from.id} in chat ${activeChat._id}`);
    } catch (error) {
      logger.error('Error sending translation result:', error);
      throw error;
    }
  }

  /**
   * Handle audio messages (MP3, etc.)
   */
  async handleAudio(ctx) {
    try {
      const userId = ctx.from.id;
      logger.info(`Processing audio message from user ${userId}`);
      
      // For now, redirect to voice handler
      // In the future, you might want to handle different audio formats differently
      await ctx.reply('üéµ –ë—É–¥—å –ª–∞—Å–∫–∞, –Ω–∞–¥—ñ—à–ª—ñ—Ç—å –≥–æ–ª–æ—Å–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è (–Ω–µ –∞—É–¥—ñ–æ —Ñ–∞–π–ª) –¥–ª—è –∫—Ä–∞—â–æ—ó —è–∫–æ—Å—Ç—ñ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è.');
    } catch (error) {
      logger.error('Error handling audio message:', error);
      await ctx.reply('‚ùå –í–∏–Ω–∏–∫–ª–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –∞—É–¥—ñ–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.');
    }
  }

  /**
   * Clean up temporary audio file
   */
  async cleanupAudioFile(filePath) {
    try {
      if (await fs.pathExists(filePath)) {
        await fs.remove(filePath);
        logger.info(`Cleaned up audio file: ${filePath}`);
      }
    } catch (error) {
      logger.error('Error cleaning up audio file:', error);
    }
  }

  /**
   * Handle document messages (in case user sends audio as document)
   */
  async handleDocument(ctx) {
    try {
      const document = ctx.message.document;
      
      // Check if it's an audio file
      if (document.mime_type && document.mime_type.startsWith('audio/')) {
        await ctx.reply('üéµ –Ø –æ—Ç—Ä–∏–º–∞–≤ –∞—É–¥—ñ–æ —Ñ–∞–π–ª, –∞–ª–µ –¥–ª—è –∫—Ä–∞—â–æ—ó —è–∫–æ—Å—Ç—ñ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –Ω–∞–¥—Å–∏–ª–∞—Ç–∏ –≥–æ–ª–æ—Å–æ–≤—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —á–µ—Ä–µ–∑ –º—ñ–∫—Ä–æ—Ñ–æ–Ω –≤ Telegram.');
        
        // You could implement document audio processing here if needed
        // For now, we'll just suggest using voice messages
      } else {
        await ctx.reply('üìÑ –Ø –æ–±—Ä–æ–±–ª—è—é –ª–∏—à–µ –≥–æ–ª–æ—Å–æ–≤—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è. –ë—É–¥—å –ª–∞—Å–∫–∞, –Ω–∞–¥—ñ—à–ª—ñ—Ç—å –≥–æ–ª–æ—Å–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É.');
      }
    } catch (error) {
      logger.error('Error handling document:', error);
      await ctx.reply('‚ùå –í–∏–Ω–∏–∫–ª–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∞.');
    }
  }
}

module.exports = new AudioHandler(); 